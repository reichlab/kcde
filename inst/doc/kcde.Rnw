%\VignetteIndexEntry{The kcde package}
%\VignetteEngine{knitr::knitr}

\documentclass[fleqn]{article}

\usepackage{geometry}
\geometry{letterpaper, top=1.5cm, left=2cm, right=2cm}

\usepackage{amssymb, amsmath, amsfonts}


\include{GrandMacros}
\newcommand{\cdf}{{c.d.f.} }
\newcommand{\pdf}{{p.d.f.} }
\newcommand{\ind}{\mathbb{I}}

\begin{document}

<<InitialBlock, echo = FALSE>>=
library(knitr)
opts_knit$set(concordance=TRUE)
@

%\maketitle

\section{Introduction}
\label{sec:Intro}

Conditional, mixed continuous and discrete, non-diagonal bw, smoothing, periodic
kernel specification


\section{Method Description}
\label{sec:MethodDescription}

Suppose we observe $\bz_t = \{z_{t,1}, \ldots, z_{t,D}\} \in \mathbb{R}^D$ at each point in time $t = 1, \ldots, T$.  Our goal is to obtain a predictive distribution for one of the observed variables, with index $d_{pred} \in \{1, \ldots, D\}$, over a range of prediction horizons contained in the set $\mathcal{P}$.  For example, if we have weekly data and we are interested in obtaining predictions for a range between 4 and 6 weeks after the most recent observation then $\mathcal{P} = \{4, 5, 6\}$.  Let $P$ be the largest element of the set $\mathcal{P}$ of prediction horizons.

In order to perform prediction, we will use lagged observations.  Let $\bl^{max} = (l^{max}_1, \ldots, l^{max}_D)$ specify the maximum number of lags for each observed variable that may be used for prediction, and let $L = \max{d} l^{max}_d$ be the overall largest lag that may be used across all variables.  In the estimation procedure we describe in Section~\ref{sec:Estimation}, we will select a subset of these lags to actually use in the predictions.  We capture which lags are actually used in the vector 
\begin{align*}
&\bu = (u_{1,0}, \ldots, u_{1, l^{max}_1}, \ldots, u_{D,0}, \ldots, u_{D, l^{max}_D}) \text{, where} \\
&u_{d, l} = \begin{cases} 0 \text{ if lag $l$ of variable $d$ is not used in forming predictions} \\ 1 \text{ if lag $l$ of variable $d$ is used in forming predictions.} \end{cases}
\end{align*}

By analogy with the standard notation in autoregressive models, we define
\begin{align*}
&\by_t = (z_{t, d_{pred}}, \ldots, B^{(P - 1)} z_{t, d_{pred}}) \text{ and} \\
&\bx_t = (B^{(P)} z_{t, 1}, \ldots, B^{(P + l^{max}_1 - 1)} z_{t, 1}, \ldots, B^{(P)} z_{t, D}, \ldots, B^{(P + l^{max}_D - 1)} z_{t, D})
\end{align*}
Here, $B^{(a)}$ is the backshift operator defined by $B^{(a)} z_{t, d} = z_{t - a, d}$.  Note that the lengths of $\by_t$ and $\bx_t$, as well as exactly which lags are used to form them, depend on $\mathcal{P}$ and $\bl^{max}$; we suppress this dependence in the notation for the sake of clarity.  The vector $\by_t$ represents the prediction target when our most recent observation was made at time $t - P$: the vector of observed values at each prediction horizon $p \in \mathcal{P}$.  The variable $\bx_t$ represents the vector of all lagged covariates that are available for use in performing prediction.

To make the notation concrete, suppose that $\bz_t$ contains the observed case count for week $t$ in San Juan, the observed case count for week $t$ in Iquitos, and the date on Monday of week $t$, and our goal is to predict the weekly case count in San Juan.  Then $D = 3$ and $d_{pred} = 1$.  If we want to predict the weekly case counts for the two weeks after the most recently observation, then $p = 2$.  If we specify that the model may include the two most recent observations for the case counts in San Juan and Iquitos, but only the time index at the most recent observation then $\bl^{max} = (1, 1, 0)$.  If our current model uses only the most recently observed case counts for San Juan and Iquitos then $\bu = (1, 0, 1, 0, 0)$, where the 1's are in the positions of the $\bu$ vector representing lag 0 of the counts for San Juan and lag 0 of the counts for Iquitos.  The variable $y_t^{(P)}$ is a vector containing the observed case counts for San Juan in weeks $t + 1$ and $t + 2$; $\bx_t^{(\bl^{max})}$ contains the observed case counts for San Juan and Iquitos in weeks $t$ and $t - 1$ as well as the time index variable in week $t$.

In order to perform prediction, we regard $\{(\by_t, \bx_t), t = 1 + P + L, \ldots, T\}$ as a sample from the joint distribution of $(\bY, \bX)$.  We wish to estimate the conditional distribution of $\bY | \bX$.  In order to do this, we employ kernel density estimation.  Let $K^{\bY}(\by, \by^*, H^{\bY})$ and $K^{\bX}(\bx, \bx^*, H^{\bX})$ be kernel functions centered at $\by^*$ and $\bx^*$ respectively and with bandwidth matrices $H^{\bY}$ and $H^{\bX}$.  We estimate the conditional distribution of $\bY | \bX$ as follows:
\begin{align}
&\widehat{f}_{\bY|\bX}(\by | \bX = \bx) = \frac{\widehat{f}_{\bY, \bX}(\by, \bx)}{\widehat{f}_{\bX}(\bx)} \label{eqn:KDECondDef} \\
&\qquad = \frac{\sum_{t \in \tau} K^{\bY, \bX}\{(\by, \bx), (\by_t, \bx_t), H^{\bY, \bX}\}}{\sum_{t \in \tau} K^{\bX}(\bx, \bx_t, H^{\bX}) } \label{eqn:KDESubKDEJtMarginal} \\
&\qquad = \frac{\sum_{t \in \tau} K^{\bY | \bX}(\by, \by_t | \bx, \bx_t, H^{\bY, \bX}) K^{\bX}(\bx, \bx_t, H^{\bX})}{\sum_{t \in \tau} K^{\bX}(\bx, \bx_t, H^{\bX}) } \label{eqn:KDESubKDEJtMarginal} \\
&\qquad = \sum_{t \in \tau} w_t K^{\bY | \bX}(\by, \by_t | \bx, \bx_t, H^{\bY, \bX}) \text{, where} \label{eqn:KDEwt} \\
&w_t = \frac{ K^{\bX}(\bx, \bx_t, H^{\bX}) }{\sum_{t^* \in \tau} K^{\bX}(\bx, \bx_{t^*}, H^{\bX}) } \label{eqn:KDEWeightsDef}
\end{align}

In Equation~\eqref{eqn:KDECondDef}, we are simply making use of the fact that the conditional density for $\bY | \bX$ can be written as the quotient of the joint density for $(\bY, \bX)$ and the marginal density for $\bX$.  In Equation~\eqref{eqn:KDESubKDEJtMarginal}, we obtain separate kernel density estimates for the joint and marginal densities in this quotient.  In Equation~\eqref{eqn:KDEwt}, we rewrite this quotient by passing the denominator of Equation~\eqref{eqn:KDESubKDEJtMarginal} into the summation in the numerator.  We can interpret the result as a weighted kernel density estimate, where each observation $t \in \tau$ contributes a different amount to the final conditional density estimate.  The amount of the contribution from observation $t$ is given by the weight $w_t$, which effectively measures how similar $\bx_t$ is to the point $\bx$ at which we are estimating the conditional density.  If $\bx_t^{(\bl^{max})}$ is similar to $\bx_{t^*}^{(\bl^{max})}$, a large weight is assigned to $t$; if $\bx_t^{(\bl^{max})}$ is different from $\bx_{t^*}^{(\bl^{max})}$, a small weight is assigned to $t$.

In kernel density estimation, it is generally required that the kernel functions integrate to $1$ in order to obtain valid density estimates.  However, after conditioning on $\bX$, it is no longer necessary that $K^{\bX}(\bx, \bx_t, H^{\bX})$ integrate to $1$.  In fact, as can be seen from Equation~\eqref{eqn:KDEWeightsDef}, any multiplicative constants of proportionality will cancel out when we form the observation weights.  We can therefore regard $K^{\bX}(\bx, \bx_t, H^{\bX})$ as a more general weighting function that measures the similarity between $\bx$ and $\bx_t$.  As we will see, eliminating the constraint that $K^{\bX}$ integrates to $1$ is a useful expansion the space of functions that can be used in calculating the observation weights.  However, we still require that $K^{\bY}$ integrates to $1$.

In Equations \eqref{eqn:KDECondDef} through \eqref{eqn:KDEWeightsDef}, $\tau$ is an index set of time points used in obtaining the density estimate.  In most settings, we can take $\tau = \{1 + P + L, \ldots, T\}$.  These are the time points for which we can form the lagged observation vector $\bx_t$ and the prediction target vector $\by_t$.  However, we will place additional restrictions on the time points included in $\tau$ in the cross-validation procedure discussed in Section \ref{sec:Estimation}.

If we wish to obtain point predictions, we can use a summary of the predictive density.  For example, if we take the expected value, we obtain kernel regression:
\begin{align}
&(\widehat{\bY} | \bX = \bx) = \mathbb{E}_{\widehat{f}_{\bY|\bX}}\{\bY | \bX = \bx\} \label{eqn:PtPredDef} \\
&\qquad = \int \sum_{t \in \tau} w_t K^{\bY}(\by, \by_t, H^{\bY}) \by \, d \by  \label{eqn:PtPredKDE} \\
&\qquad = \sum_{t \in \tau} w_t \by_t  \label{eqn:PtPredFinal}
\end{align}
The equality in Equation~\eqref{eqn:PtPredFinal} holds if the kernel function $K^{\bY}(\by, \by_t, H^{\bY})$ is symmetric about $\by_t$, or more generally if it is the pdf of a random variable with expected value $\by_t$.

Another alternative that we pursue is the use of smoothed observations in forming the lagged observation vectors.  We use smoothed case counts on a log scale for the weighting kernels, and the unsmoothed case counts on the original scale for the prediction kernels.

\section{Parameter Estimation}
\label{sec:Estimation}

We use cross-validation to select the variables that are used in the model and estimate the corresponding bandwidth parameters by (approximately) minimizing a cross-validation measure of the quality of the predictions obtained from the model.  Formally,
\begin{align}
&(\widehat{\bu}, \widehat{H}^{\bX}, \widehat{H}^{\bY}) \approx \argmin{(\bu, H^{\bX}, H^{\bY})} \sum_{t^* = 1 + P + L}^T Q[ \by_{t^*}, \widehat{f}(\by | \bX = \bx_{t^*} ; \bu, H^{\bX}, H^{\bY}, \{ (\by_t, \bx_t): t \in \tau_{t^*} \}) ] \label{eqn:ParamEst}
\end{align}
Here, $Q$ is a loss function that measures the quality of the estimated density $\widehat{f}$ given an observation $\by_{t^*}$.  We have made the dependence of this estimated density on the the parameters $\bu$, $H^{\bx}$, and $H^{\bY}$, as well as on the data $\{ (\by_t, \bx_t): t \in \tau_{t^*} \}$, explicit in the notation.  In order to reduce the potential for our parameter estimates to be affected by local correlation in the time series, we eliminate all time points that fall within one year of $t^*$ from the index set $\tau_{t^*}$ used to form the conditional density estimate $\widehat{f}(\by | \bX = \bx_{t^*} ; \bu, H^{\bX}, H^{\bY}, \{ (\by_t, \bx_t): t \in \tau_{t^*} \})$.

\lboxit{Talk about proper scoring rules and our particular choice of $Q$.}

We use a forward/backward stagewise procedure to obtain the set of combinations of variables and lags that are included in the final model (represented by $\bu$).  For each candidate model, we use the limited memory box constrained optimization procedure of \cite{byrd1995limitedmemoryoptim} to estimate the bandwidth parameters.  The approximation in Equation~\eqref{eqn:ParamEst} is due to the fact that this optimization procedure may not find a global minimum.


\section{Examples}
\label{sec:Examples}

In this Section, we illustrate the methods through applications to prediction in
examples with several real time series data sets.

\subsection{Example 1: Influenza Prediction}

In our first and simplest example, we apply the method for prediction of
influenza with prediction horizons of 1 through 4 weeks.  Data on influenza
incidence are available through {\tt R}'s {\tt cdcfluview} package.  Here we
create a data set with a nationally aggregated measure of flu incidence

<<FluDataLoadData, echo = TRUE>>=
library(cdcfluview)
library(dplyr)
library(lubridate)
library(ggplot2)
library(grid)
library(kcde)

usflu<-get_flu_data("national", "ilinet", years=1997:2015)
ili_national <- transmute(usflu,
    region.type = REGION.TYPE,
    region = REGION,
    year = YEAR,
    week = WEEK,
    total_cases = as.numeric(X..WEIGHTED.ILI))
ili_national$time <- ymd(paste(ili_national$year, "01", "01", sep = "-"))
week(ili_national$time) <- ili_national$week
ili_national$time_index <- seq_len(nrow(ili_national))

str(ili_national)
@

We plot the {\tt total\_cases} measure over time, representing missing values
with vertical grey lines.  The low season was not measured in the first few
years.

<<FluDataInitialPlotTotalCases, echo = TRUE>>=
ggplot() +
    geom_line(aes(x = as.Date(time), y = total_cases), data =
ili_national) +
    geom_vline(aes(xintercept = as.numeric(as.Date(time))),
        colour = "grey",
        data = ili_national[is.na(ili_national$total_cases), ]) +
    scale_x_date() +
    xlab("Time") +
    ylab("Total Cases") +
    theme_bw()
@

There are several methods that we could employ to handle these missing data:
\begin{enumerate}
\item Impute the missing values.  They are all in the low season, so this should be relatively easy to do.
\item Drop all data up through the last NA.
\item Use the data that are available.
\end{enumerate}
Of these approaches, the first is probably preferred.  The concern with the second
is that we are not making use of all of the available data.  The potential concern with the
third is that in the data used in estimation, there will be more examples of prediction of values in the high season
using values in the high season and middle of the season than of prediction of values in the high season using values in the low season.
This could potentially affect our inference.  However, we do not expect this effect to be large,
so we proceed with this option for the purposes of this example.

We also plot histograms of the observed total cases on the original scale and on the log scale.

<<FluDataHistogramPlotTotalCases, echo = TRUE>>=
hist_df <- rbind(
	data.frame(value = ili_national$total_cases,
    	variable = "Total Cases"),
    data.frame(value = log(ili_national$total_cases),
    	variable = "log(Total Cases)")
)

ggplot(aes(x = value), data = hist_df) +
    geom_histogram() +
    facet_wrap( ~ variable, ncol = 2) +
    xlab("Total Cases") +
    theme_bw()
@

These plots demonstrate that total cases follows an approximately log-normal
distribution.  In the application below, we will consider modeling these data on
both the original scale and the log scale.  Intuitively, since we are using a
kernel that is obtained from a Gaussian, modeling the data on the log scale
should yield better performance.  On the other hand, the performance gain may be
negligible if we have enough data.

Finally, we plot the autocorrelation function:

<<FluDataACFPlotTotalCases, echo = TRUE>>=
last_na_ind <- max(which(is.na(ili_national$total_cases)))
non_na_inds <- seq(from = last_na_ind + 1, to=nrow(ili_national))
acf(ili_national$total_cases[non_na_inds],
  lag.max = 52 * 4)
@

This plot illustrates the annual periodicity that was also visible in the
initial data plot above.  There is no apparent evidence of longer term annual
cycles.  We therefore include a periodic kernel acting on the time index with a
period of 52.2 weeks (the length of the period is motivated by the fact that
in our data, there is a year with 53 weeks once every 5 or 6 years).

We now do some set up for estimation and prediction with kcde.  First, we 
create a list with parameters that specify the kernel function components.

<<FluDataKernelComponentsSetup, echo = TRUE>>=
## Definitions of kernel components.  A couple of notes:
##   1) In the current implementation, it is required that separate kernel
##      components be used for lagged (predictive) variables and for leading
##      (prediction target) variables.
##   2) The current syntax is verbose; in a future version of the package,
##      convenience functions may be provided.

## Define kernel components -- 3 pieces:
##   1) Periodic kernel acting on time index
##   2) pdtmvn kernel acting on lagged total cases (predictive) -- all continuous
##   3) pdtmvn kernel acting on lead total cases (prediction target) -- all continuous
kernel_components <- list(
    list(
        vars_and_offsets = data.frame(var_name = "time_index",
            offset_value = 0L,
            offset_type = "lag",
            combined_name = "time_index_lag0",
            stringsAsFactors = FALSE),
        kernel_fn = periodic_kernel,
        theta_fixed = list(period=pi / 52.2),
        theta_est = list("bw"),
        initialize_kernel_params_fn = initialize_params_periodic_kernel,
        initialize_kernel_params_args = NULL,
        vectorize_kernel_params_fn = vectorize_params_periodic_kernel,
        vectorize_kernel_params_args = NULL,
        update_theta_from_vectorized_theta_est_fn = update_theta_from_vectorized_theta_est_periodic_kernel,
        update_theta_from_vectorized_theta_est_args = NULL
    ),
    list(
        vars_and_offsets = data.frame(var_name = "total_cases",
            offset_value = 1L,
            offset_type = "horizon",
            combined_name = "total_cases_horizon1",
            stringsAsFactors = FALSE),
        kernel_fn = pdtmvn_kernel,
        rkernel_fn = rpdtmvn_kernel,
        theta_fixed = list(
            parameterization = "bw-diagonalized-est-eigenvalues",
            continuous_vars = "total_cases_horizon1",
            discrete_vars = NULL,
            discrete_var_range_fns = NULL,
            lower = -Inf,
            upper = Inf
        ),
        theta_est = list("bw"),
        initialize_kernel_params_fn = initialize_params_pdtmvn_kernel,
        initialize_kernel_params_args = NULL,
        vectorize_kernel_params_fn = vectorize_params_pdtmvn_kernel,
        vectorize_kernel_params_args = NULL,
        update_theta_from_vectorized_theta_est_fn = update_theta_from_vectorized_theta_est_pdtmvn_kernel,
        update_theta_from_vectorized_theta_est_args = NULL
    ))
#,
#    list(
#        vars_and_lags = vars_and_lags[3:5, ],
#        kernel_fn = pdtmvn_kernel,
#        rkernel_fn = rpdtmvn_kernel,
#        theta_fixed = NULL,
#        theta_est = list("bw"),
#        initialize_kernel_params_fn = initialize_params_pdtmvn_kernel,
#        initialize_kernel_params_args = list(
#            continuous_vars = vars_and_lags$combined_name[3:4],
#            discrete_vars = vars_and_lags$combined_name[5],
#            discrete_var_range_fns = list(
#                c_lag2 = list(a = pdtmvn::floor_x_minus_1, b = floor, in_range = pdtmvn::equals_integer, discretizer = round_up_.5))
#        ),
#        vectorize_theta_est_fn = vectorize_params_pdtmvn_kernel,
#        vectorize_theta_est_args = NULL,
#        update_theta_from_vectorized_theta_est_fn = update_theta_from_vectorized_theta_est_pdtmvn_kernel,
#        update_theta_from_vectorized_theta_est_args = list(
#            parameterization = "bw-diagonalized-est-eigenvalues"
#        )
#    ))
@

Next, we create a list with parameters controlling how estimation is performed.

<<FluDataKCDESetup, echo=TRUE>>=
kcde_control <- create_kcde_control(X_names = "time_index",
    y_names = "total_cases",
    time_name = "time",
    prediction_horizons = 1L,
    kernel_components = kernel_components,
    crossval_buffer = ymd("2010-01-01") - ymd("2009-01-01"),
    loss_fn = neg_log_score_loss,
    loss_fn_prediction_args = list(
        prediction_type = "distribution",
        log = TRUE),
    filter_control <- NULL,
    loss_args = NULL,
    prediction_inds_not_included = c())
@

We are now ready to estimate the bandwidth parameters, using data up through
2014
<<FluDataKCDEEstimation, echo=TRUE>>=
# flu_kcde_fit_orig_scale <- kcde(data = ili_national[ili_national$year <= 2014, ],
#    kcde_control = kcde_control)
@

There are several methods available for examining the predictive distribution. 
Here we simply draw a monte carlo sample from the predictive distribution for
total cases in the first week of 2015. Then we plot a representation of this
sample against the observed value for that week.

<<FluDataKCDEPredictiveSampleAndPlot, echo=TRUE>>=
# predictive_sample <- kcde_predict(kcde_fit = flu_kcde_fit_orig_scale,
#         prediction_data = ili_national[ili_national$year == 2014 & ili_national$week == 53, , drop = FALSE],
#         leading_rows_to_drop = 0,
#         trailing_rows_to_drop = 1L,
#         additional_training_rows_to_drop = NULL,
#         prediction_type = "sample",
#         n = 10000L)

# ggplot() +
# 	geom_density(aes(x = predictive_sample)) +
# 	geom_vline(aes(xintercept = ili_national$total_cases[ili_national$year == 2015 & ili_national$week == 1]),
# 		colour = "red") +
# 	xlab("Total Cases") +
# 	ylab("Predictive Density") +
# 	ggtitle("Realized total cases vs. one week ahead predictive density\nWeek 1 of 2015") +
# 	theme_bw()
@



\section{Publications}
\label{sec:Publications}

\begingroup
\renewcommand{\section}[2]{}
\bibliographystyle{plainnat}
\bibliography{SSRbib}
\endgroup


\end{document}